# global variables in imported modules remain separate
# in different DistAlgo processes
import ledger
import blocktree
import pacemaker
from collections import deque
import mempool
import leaderelection
import safety
import random
import pickle
import logging
import syncmanager

class Replica(process):
    def setup(
        replica_ids,
        client_ids,
        replicas,
        clients,
        transmission_delay_bound,
        public_keys,
        private_key,
        id,
        seed,
        server_ids,
        num_faulty,
        testcase_name,
        isFI = False
    ):
        self.id_to_server = {server_ids[server]: server for server in server_ids.keys()}
        self.u = to_str()
        safety.public_keys = public_keys
        safety.private_key = private_key
        safety.u = to_str()
        ledger.open_ledger(testcase_name + "-" + to_str())
        ledger.validators = len(replica_ids)

        recent_proposals = deque()
        leaderelection.validators = sorted(list(replica_ids))
        leaderelection.exclude_size = num_faulty
        pacemaker.transmission_delay_bound = transmission_delay_bound
        pacemaker.u = to_str()
        pacemaker.f = num_faulty
        pacemaker.broadcast = broadcast
        blocktree.u = to_str()
        blocktree.f = num_faulty
        mempool.u = to_str()
        mempool.id_to_server = self.id_to_server
        mempool.multicast = multicast
        ledger.u = to_str()
        random.seed(seed)
        syncmanager.u = to_str()
        syncmanager.multicast = multicast
        syncmanager.process_certificate_qc = process_certificate_qc
        syncmanager.handle_msg = handle_msg
    
        if not isFI:
            logging.basicConfig(filename = f"log/{testcase_name}_{to_str()}.log",level=logging.INFO,
                format= '[%(asctime)s] - ' + '[' + to_str() + '] - ' + '%(message)s',filemode='w') 
            pacemaker.logging = logging
            ledger.logging = logging
            safety.logging = logging

            logging.info(f"[Initial settings for current replica]: \n\
            public keys are: {public_keys}\n\
            private key is: {private_key}\n\
            random seed is: {seed}\n\
            total replicas are: {replicas}\n\
            transmission delay bound is: {transmission_delay_bound}")
        
    def multicast(msg, to):
        if isinstance(to, set):
            for dst in to:
                multicast(msg, to=dst)
            return
        if isinstance(to, str):
            to = self.id_to_server[to]
        send(msg, to=to)
        logging.info(f"[send '{msg[0]}' message] to [{to}] \n\
            content={msg}")
    
    # __str__ doesn't seem to work with DistAlgo...
    def to_str():
        return 'replica' + str(id)

    # loop: wait for next event M ; Main.start_event_processing(M)
    # if M is a local timeout then Pacemaker.local_timeout_round()
    def run():
        output('started running')
        logging.info("[started running]")
        process_new_round_event(None)
        pacemaker.start_timer(0)
        while True:
            (duration, paused) = pacemaker.get_round_timer(pacemaker.current_round)
            if await received(('done',)):
                break
            elif timeout(duration):
                (duration, paused) = pacemaker.get_round_timer(pacemaker.current_round)
                if not(paused):
                    pacemaker.local_timeout_round()
        ledger.close_ledger()
        output('finished running')
        logging.info("[finished running]")
    
    # Procedure start event processing(M)
    # │ if M is a proposal message then process_proposal_msg(M)
    # │ if M is a vote message then process_vote_msg(M)
    # └ if M is a timeout message then process_timeout_msg(M)
    def receive(msg=M, from_=sender):
        handle_msg(M, sender)

    # indirectly called so we can replay messages from syncmanager
    def handle_msg(M, sender):
        msg_type = M[0]
        sender_desc = server_ids[sender] if sender in server_ids else 'Runner'
        logging.info(f"[received '{M[0]}' message] from: [{sender_desc}] \n\
            content = [{M}]")
        if msg_type == 'proposal':
            output('%s received %s message from %s for round %d' % (to_str(), M[0], server_ids[sender], M[1].block.round))
            process_proposal_msg(M[1], sender)
        elif msg_type == 'vote':
            output('%s received %s message from %s for round %d' % (to_str(), M[0], server_ids[sender], M[1].vote_info.round))
            process_vote_msg(M[1], sender)
        elif msg_type == 'timeout':
            process_timeout_msg(M[1], sender)
        elif msg_type == 'sync_request':
            syncmanager.process_sync_request(M[1], sender)
        elif msg_type == 'sync_response':
            syncmanager.process_sync_response(M[1], sender)
        elif msg_type == 'request' and sender in clients:
            # process request from client
            txn = M[1]

            author = txn[1]
            signature = M[2]

            if not safety.verify_client(author, signature):
                # invalid signature, reject the request
                pass
            # check if duplicate (otherwise add to mempool.pending)
            commit_state_id = mempool.check_transaction(txn, sender)
            if commit_state_id:
                # duplicate request, reply to client
                output('sending reply to client %s: %s' % (('committed', txn, commit_state_id), sender))
                logging.info('sending reply to client %s: %s' % (('committed', txn, commit_state_id), sender))
                send(('committed', txn, commit_state_id), to=sender)
    
    # Procedure process_certificate_qc(qc)
    # │ Block-Tree.process_qc(qc)
    # │ LeaderElection.update_leaders(qc)
    # └ Pacemaker.advance_round(qc.vote_info.round) 
    def process_certificate_qc(qc):
        if qc != None:
            committed_payload, commit_state_id = blocktree.process_qc(qc)
            if committed_payload != None:
                mempool.commit_transactions(
                    committed_payload,
                    commit_state_id
                )
            leaderelection.update_leaders(qc)
            pacemaker.advance_round_qc(qc.vote_info.round)

    # Procedure process_proposal_msg(P)
    # │ process_certificate_qc(P.block.qc)
    # │ process_certificate_qc(P.high_commit_qc)
    # │ Pacemaker.advance_round_tc(P.last_round_tc)
    # │ round ← Pacemaker.current_round
    # │ leader ← LeaderElection.get_leader(current_round)
    # │ if P.block.round ≠ round V P.sender ≠ leader V P.block.author ≠ leader then
    # │ └ return
    # │ Block-Tree.execute_and_insert(P) // Adds a new speculative state to the Ledger
    # │ vote_msg ← Safety.make_vote(P.block, P.last_round_tc)
    # │ if vote_msg ≠ ⊥ then
    # └ └ send vote_msg to LeaderElection.get_leader(current_round + 1)
    def process_proposal_msg(P, sender):
        if syncmanager.need_sync(P.high_commit_qc, P.block.qc, P, sender):
            output('%s has started syncing' % to_str())
            return
        process_certificate_qc(P.block.qc)
        process_certificate_qc(P.high_commit_qc)
        pacemaker.advance_round_tc(P.last_round_tc)
        current_round = pacemaker.current_round
        leader = leaderelection.get_leader(current_round)
        if P.block.round != current_round or \
            self.server_ids.get(sender) != leader or \
            P.block.author != leader:
            return
        blocktree.execute_and_insert(P.block)
        vote_msg = safety.make_vote(P.block, P.last_round_tc)
        if vote_msg:
            multicast(('vote', vote_msg), to=leaderelection.get_leader(current_round + 1))

    def process_timeout_msg(M, sender):
        output("%s processing timeout" % to_str())
        if not syncmanager.need_sync(M.high_commit_qc, M.tmo_info.high_qc, M, sender):
            process_certificate_qc(M.tmo_info.high_qc)
            process_certificate_qc(M.high_commit_qc)
        # we can still handle the qc's in the TC even if we aren't synced up to them
        # because we don't put these qc's in blocktree until they are proposed in future rounds
        pacemaker.advance_round_tc(M.last_round_tc)
        tc = pacemaker.process_remote_timeout(M)
        if tc:
            pacemaker.advance_round_tc(tc)
            process_new_round_event(tc)
        output("%s done processing timeout" % to_str())

    def process_vote_msg(M, sender):
        # TODO technically if we are missing the block voted on in the previous round we need to request that with a sync request
        # but that's a new case cause there's no QC for that block yet...
        if syncmanager.need_vote_sync(M.high_commit_qc, M.vote_info, M, sender):
            return
        qc = blocktree.process_vote(M)
        if qc:
            process_certificate_qc(qc)
            process_new_round_event(None)

    def process_new_round_event(last_tc):
        if to_str() == leaderelection.get_leader(pacemaker.current_round):
            output('%s is now leader for current round %d' % (to_str(), pacemaker.current_round))
            if last_tc:
                output('\tround %d ended in timeout' % last_tc.round)
            logging.info(f'[{to_str()} is now the leader for current round {pacemaker.current_round}]')
            # TODO in rust implementation, blockstore has path_from_commit_root method
            # which returns list of uncommitted but proposed transactions from the last committed
            # block to the currently proposed block (i think?)
            # this is needed to tell mempool what txns are already proposed and in consensus
            # so we don't propose a duplicate txn
            # so something like this:
            exclude_payloads = [block.payload for block in blocktree.path_from_commit_root()]
            transactions = mempool.get_transactions(exclude_payloads)
            if len(transactions) > 0 or blocktree.pending_transaction():
                b = blocktree.generate_block(
                    transactions,
                    pacemaker.current_round
                )
                output('proposing txns:', transactions)
                broadcast(('proposal', blocktree.ProposalMsg(b, last_tc, blocktree.high_commit_qc)))
            else:
                output(to_str() + ' has no reason to generate proposal')

    def broadcast(msg):
        multicast(msg, to=replica_ids)
