# global variables in imported modules remain separate
# in different DistAlgo processes
import ledger
from datetime import datetime
import blocktree
import pacemaker
from collections import deque
import mempool
import leaderelectiontestexec as leaderelection
import safety
import random
import pickle
import syncmanager
from logger import createLogger

class Replica(process):
    def setup(
        executor, # only passed in here so we can send committed log messages
        replica_ids,
        transmission_delay_bound,
        public_keys,
        private_key,
        ident,
        seed,
        num_faulty,
        testcase_name,
        exec_config_name,
        bugs
    ):
        self.done = False
        self.u = to_str()
        safety.public_keys = public_keys
        safety.private_key = private_key
        safety.u = to_str()
        safety.bugs = bugs
        ledger.open_ledger('ledger/' + exec_config_name + '/' + testcase_name + "-" + to_str())
        ledger.validators = len(replica_ids)

        recent_proposals = deque()
        leaderelection.validators = sorted(list(replica_ids))
        leaderelection.exclude_size = num_faulty
        pacemaker.transmission_delay_bound = transmission_delay_bound
        pacemaker.u = to_str()
        pacemaker.f = num_faulty
        pacemaker.broadcast = broadcast
        pacemaker.bugs = bugs
        blocktree.u = to_str()
        blocktree.f = num_faulty
        blocktree.bugs = bugs
        mempool.u = to_str()
        mempool.multicast = multicast
        ledger.u = to_str()
        ledger.bugs = bugs
        random.seed(seed)
        syncmanager.u = to_str()
        syncmanager.multicast = multicast
        syncmanager.process_certificate_qc = process_certificate_qc
        syncmanager.handle_msg = handle_msg
        syncmanager.bugs = bugs
        # logging.basicConfig(filename = f"log/{testcase_name}_{to_str()}.log",level=logging.INFO,
        #     format= '[%(asctime)s] - ' + '[' + to_str() + '] - ' + '%(message)s',filemode='w') 
        formatter = to_str()
        self.logger = createLogger(__name__, 'log/' + exec_config_name + '/%s_%s.log' % (testcase_name, to_str()), formatter)
        pacemaker.logger = self.logger
        ledger.logger = self.logger
        safety.logger = self.logger
        blocktree.logger = self.logger
        syncmanager.logger = self.logger

        logger.debug(f"[Initial settings for current replica]: \n\
        public keys are: {public_keys}\n\
        private key is: {private_key}\n\
        random seed is: {seed}\n\
        total replicas are: {replica_ids}\n\
        transmission delay bound is: {transmission_delay_bound}")

        
    def setattr(attr, val):
        logger.info('%s called setattr with attr %s and val %s' % (u, attr, str(val)))
        if attr == 'last_vote_round':
            safety.last_vote_round = val
        elif attr == 'current_round':
            pacemaker.current_round = val
        elif attr == 'leader':
            leaderelection.reputation_leaders[pacemaker.current_round + 1] = val

    def multicast(msg, to):
        if isinstance(to, set) or isinstance(to, list):
            for dst in to:
                multicast(msg, to=dst)
            return
        send(msg, to=to)
        logger.debug(f"[send '{msg[0]}' message] to [{to}] \n\
            content={msg}")
    
    # __str__ doesn't seem to work with DistAlgo...
    def to_str():
        return ident

    # loop: wait for next event M ; Main.start_event_processing(M)
    # if M is a local timeout then Pacemaker.local_timeout_round()
    # TODO stop timeout loops
    def run():
        logger.debug("[started running]")
        process_new_round_event(None)
        pacemaker.start_timer(0)
        while True:
            (duration, paused) = pacemaker.get_round_timer(pacemaker.current_round)
            if await done:
                break
            elif timeout(duration):
                (duration, paused) = pacemaker.get_round_timer(pacemaker.current_round)
                if not(paused):
                    pacemaker.local_timeout_round()
        ledger.close_ledger()
        logger.info("%s finished running" % u)
    
    # Procedure start event processing(M)
    # │ if M is a proposal message then process_proposal_msg(M)
    # │ if M is a vote message then process_vote_msg(M)
    # └ if M is a timeout message then process_timeout_msg(M)
    # indirectly called so we can replay messages from syncmanager
    # and receive messages from replicatestexec
    def handle_msg(M, sender):
        msg_type = M[0]
        logger.debug(f"[received '{M[0]}' message] from: [{sender}] \n\
            content = [{M}]")
        if msg_type == 'proposal':
            logger.info('%s received %s message from %s for round %d' % (to_str(), M[0], sender, M[1].block.round))
            process_proposal_msg(M[1], sender)
        elif msg_type == 'vote':
            logger.info('%s received %s message from %s for round %d because it is leader for round %d' % (to_str(), M[0], sender, M[1].vote_info.round, M[1].vote_info.round + 1))
            process_vote_msg(M[1], sender)
        elif msg_type == 'timeout':
            #logger.info('%s received %s message from %s for round %d with last_tc round %d' % (to_str(), M[0], sender, M[1].tmo_info.round, M[1].last_round_tc.round if M[1].last_round_tc else -1))
            process_timeout_msg(M[1], sender)
        elif msg_type == 'sync_request':
            logger.info('%s received %s message from %s' % (to_str(), M[0], sender))
            syncmanager.process_sync_request(M[1], sender)
        elif msg_type == 'sync_response':
            logger.info('%s received %s message from %s for high_qc round %d' % (to_str(), M[0], sender, M[1].high_qc_path[-1].round if M[1].high_qc_path else -1))
            syncmanager.process_sync_response(M[1], sender)
        elif msg_type == 'done':
            done = True
        elif msg_type == 'request' and sender in clients:
            # process request from client
            txn = M[1]
            author = txn[1]
            signature = M[2]
            if not safety.verify_client(author, signature):
                # invalid signature, reject the request
                pass
            # check if duplicate (otherwise add to mempool.pending)
            commit_state_id = mempool.check_transaction(txn, sender)
            if commit_state_id:
                # duplicate request, reply to client
                #logger.info('sending reply to client %s: %s' % (('committed', txn, commit_state_id), sender))
                logger.debug('sending reply to client %s: %s' % (('committed', txn, commit_state_id), sender))
                multicast(('committed', txn, commit_state_id), to=sender)
    
    # Procedure process_certificate_qc(qc)
    # │ Block-Tree.process_qc(qc)
    # │ LeaderElection.update_leaders(qc)
    # └ Pacemaker.advance_round(qc.vote_info.round) 
    def process_certificate_qc(qc):
        if qc != None:
            committed_payload, commit_state_id = blocktree.process_qc(qc)
            if committed_payload != None:
                mempool.commit_transactions(
                    committed_payload,
                    commit_state_id
                )
                # tell TestExec that we have locally committed this block
                block_id = qc.vote_info.parent_id
                rnd = qc.vote_info.parent_round
                if not some(sent(('executor', ('committed', (_block_id, _, _))))):
                    send(('committed', (block_id, rnd, datetime.now())), to='executor')
            leaderelection.update_leaders(qc)
            pacemaker.advance_round_qc(qc.vote_info.round)

    # Procedure process_proposal_msg(P)
    # │ process_certificate_qc(P.block.qc)
    # │ process_certificate_qc(P.high_commit_qc)
    # │ Pacemaker.advance_round_tc(P.last_round_tc)
    # │ round ← Pacemaker.current_round
    # │ leader ← LeaderElection.get_leader(current_round)
    # │ if P.block.round ≠ round V P.sender ≠ leader V P.block.author ≠ leader then
    # │ └ return
    # │ Block-Tree.execute_and_insert(P) // Adds a new speculative state to the Ledger
    # │ vote_msg ← Safety.make_vote(P.block, P.last_round_tc)
    # │ if vote_msg ≠ ⊥ then
    # └ └ send vote_msg to LeaderElection.get_leader(current_round + 1)
    def process_proposal_msg(P, sender):
        if syncmanager.need_sync(P.high_commit_qc, P.block.qc, ('proposal', P), sender):
            pacemaker.advance_round_tc(P.last_round_tc)
            return
        process_certificate_qc(P.block.qc)
        process_certificate_qc(P.high_commit_qc)
        pacemaker.advance_round_tc(P.last_round_tc)
        current_round = pacemaker.current_round
        leader = leaderelection.get_leader(current_round)
        if P.block.round != current_round or \
            sender != leader or \
            P.block.author != leader:
            return
        blocktree.execute_and_insert(P.block)
        vote_msg = safety.make_vote(P.block, P.last_round_tc)
        if vote_msg:
            if "vote_leader_current_round" in self.bugs:
                multicast(('vote', vote_msg), to=leaderelection.get_leader(current_round))
            else:
                multicast(('vote', vote_msg), to=leaderelection.get_leader(current_round + 1))

    def process_timeout_msg(M, sender):
        if not syncmanager.need_sync(M.high_commit_qc, M.tmo_info.high_qc, ('timeout', M), sender):
            process_certificate_qc(M.tmo_info.high_qc)
            process_certificate_qc(M.high_commit_qc)
        # we can still handle the qc's in the TC even if we aren't synced up to them
        # because we don't put these qc's in blocktree until they are proposed in future rounds
        pacemaker.advance_round_tc(M.last_round_tc)
        tc = pacemaker.process_remote_timeout(M)
        if tc:
            pacemaker.advance_round_tc(tc)
            process_new_round_event(tc)

    def process_vote_msg(M, sender):
        # TODO technically if we are missing the block voted on in the previous round we need to request that with a sync request
        # but that's a new case cause there's no QC for that block yet...
        if syncmanager.need_vote_sync(M.high_commit_qc, M.vote_info, M, sender):
            logger.info('%s ignoring vote because not synced' % u)
            return
        qc = blocktree.process_vote(M)
        if qc:
            process_certificate_qc(qc)
            process_new_round_event(None)

    def process_new_round_event(last_tc):
        if to_str() == leaderelection.get_leader(pacemaker.current_round):
            logger.info('%s is now leader for current round %d' % (to_str(), pacemaker.current_round))
            if last_tc:
                logger.info('\tround %d ended in timeout' % last_tc.round)
            logger.debug(f'[{to_str()} is now the leader for current round {pacemaker.current_round}]')
            # TODO in rust implementation, blockstore has path_from_commit_root method
            # which returns list of uncommitted but proposed transactions from the last committed
            # block to the currently proposed block (i think?)
            # this is needed to tell mempool what txns are already proposed and in consensus
            # so we don't propose a duplicate txn
            # so something like this:
            exclude_payloads = [block.payload for block in blocktree.path_from_commit_root()]
            transactions = mempool.get_transactions(exclude_payloads)
            if len(transactions) > 0 or blocktree.pending_transaction():
                b = blocktree.generate_block(
                    transactions,
                    pacemaker.current_round
                )
                msg = blocktree.ProposalMsg(b, last_tc, blocktree.high_commit_qc)
                logger.info('%s sending proposal for round %d with block id %s\n\tparent block id %s' % (u, pacemaker.current_round, b.id, b.qc and b.qc.vote_info.id))
                broadcast(('proposal', msg))
            else:
                logger.info(to_str() + ' has no reason to generate proposal')

    def broadcast(msg):
        multicast(msg, to=replica_ids)
